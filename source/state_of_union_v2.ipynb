{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze state of the union addresses. \n",
    "Data source: https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents\n",
    "scrape the text of all speeches and then maybe try to find patterns of speech of each president?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "\n",
    "# Data source we are going to scrape for results\n",
    "data_url = 'https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents'\n",
    "\n",
    "link_list = []\n",
    "\n",
    "resp = urllib.request.urlopen(data_url)\n",
    "soup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "# Get all links to state of the union addresses from \n",
    "for link in soup.find_all('a', href=True):\n",
    "    if \"union_address\" in link['href'].lower() or \"union_speech\" in link['href'].lower() \\\n",
    "        and \"portal\" not in link['href'].lower() and \"#\" not in link['href'].lower():\n",
    "        link_list.append(link['href'])\n",
    "\n",
    "# extract the text of a speech from a URL\n",
    "# text is extracted in a list of paragraphs (strings) for each speech\n",
    "def get_speech(url):\n",
    "    return([ p.text.strip() for p in BeautifulSoup(urllib.request.urlopen(url)).find_all(\"p\") if \\\n",
    "             'This work is in the public domain in the United States because it is a work of the United States federal government' \\\n",
    "             not in p.text.strip()])\n",
    "\n",
    "speeches = [get_speech('https://en.wikisource.org' + link) for link in link_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract presidents names from link text\n",
    "presidents = [ link.replace('%','/').split('/')[2].replace('_',' ') for link in link_list ]\n",
    "\n",
    "# Extract state of the union text entries so we can extract the date\n",
    "sou_entries = []\n",
    "for item in soup.find_all('li'):\n",
    "    if 'union' in item.text.strip().lower() and '(' in  item.text.strip().lower():\n",
    "        sou_entries.append(item.text.strip())\n",
    "\n",
    "speeches_pd = pd.DataFrame({\n",
    "                'president' : presidents,\n",
    "                'speech' : speeches,\n",
    "                'year' : [int(re.findall('\\d+',item)[1]) for item in sou_entries ]} )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>speech</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>[Mr. Speaker, Mr. Vice President, Members of C...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Richard Nixon</td>\n",
       "      <td>[Twenty-five years ago I sat here as a freshma...</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Martin Van Buren</td>\n",
       "      <td>[Fellow-Citizens of the Senate and House of Re...</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Woodrow Wilson</td>\n",
       "      <td>[GENTLEMEN OF THE CONGRESS:, When I addressed ...</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Andrew Jackson</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Millard Fillmore</td>\n",
       "      <td>[Fellow-Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Andrew Jackson</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Franklin Delano Roosevelt</td>\n",
       "      <td>[Mr. President, Mr. Speaker, Members of the Co...</td>\n",
       "      <td>1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>John Tyler</td>\n",
       "      <td>[To the Senate and House of Representatives of...</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     president  \\\n",
       "230               Donald Trump   \n",
       "184              Richard Nixon   \n",
       "51            Martin Van Buren   \n",
       "131             Woodrow Wilson   \n",
       "45              Andrew Jackson   \n",
       "62            Millard Fillmore   \n",
       "42              Andrew Jackson   \n",
       "147  Franklin Delano Roosevelt   \n",
       "53                  John Tyler   \n",
       "7            George Washington   \n",
       "\n",
       "                                                speech  year  \n",
       "230  [Mr. Speaker, Mr. Vice President, Members of C...  2018  \n",
       "184  [Twenty-five years ago I sat here as a freshma...  1972  \n",
       "51   [Fellow-Citizens of the Senate and House of Re...  1840  \n",
       "131  [GENTLEMEN OF THE CONGRESS:, When I addressed ...  1920  \n",
       "45   [Fellow Citizens of the Senate and of the Hous...  1834  \n",
       "62   [Fellow-Citizens of the Senate and of the Hous...  1851  \n",
       "42   [Fellow Citizens of the Senate and of the Hous...  1831  \n",
       "147  [Mr. President, Mr. Speaker, Members of the Co...  1937  \n",
       "53   [To the Senate and House of Representatives of...  1842  \n",
       "7    [Fellow Citizens of the Senate and of the Hous...  1796  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_pd.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process and TfIdf Vectorize Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Convert nltk POS to POS that can be used by lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Lemmatize a word with its appropriate POS tag\n",
    "def lemmatize_with_pos(word,testing=False):\n",
    "    pos = get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "    if testing==True:\n",
    "        print('POS: ' + pos)\n",
    "    if pos != '':\n",
    "        return(lmtzr.lemmatize(word,pos))\n",
    "    else:\n",
    "        return(lmtzr.lemmatize(word))\n",
    "    \n",
    "MyStopWords = get_stop_words('en')\n",
    "MyStopWords.extend(['make','will','can','applause'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS: n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dsfs'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_pos('dsfs',testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean, Tokenize, Lemmatize, and Vectorize all speeches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each speech is stored as a list of paragraph strings. \n",
    "# Here we join the paragraphs into a single speech string\n",
    "speech_list = [\" \".join(speech) for speech in speeches_pd['speech'].tolist() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - this step may take a few minutes\n",
    "tokens = [[lemmatize_with_pos(word.lower()) for word in word_tokenize(speech) if \\\n",
    "          word.isalpha() and word.lower() not in MyStopWords ] for \\\n",
    "          speech in speech_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf Vectorize\n",
    "tvec = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf = pd.DataFrame(tvec.fit_transform([\" \".join(tok) for tok in tokens ]).toarray(),\n",
    "                    columns  = tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 16934)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=60, mean_change_tol=0.001,\n",
       "             n_components=8, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LDA Topic Modeling\n",
    "speech_lda = LatentDirichletAllocation(n_components=8,\n",
    "                                       max_iter=60,\n",
    "                                learning_method='online')\n",
    "speech_lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: state | government | may | united | upon | country | law | interest | last\n",
      "Topic #1: state | country | desideratum | upon | without | occasion | congress | united | important\n",
      "Topic #2: state | supplementation | multiply | dailiness | greek | sly | shall | june | domain\n",
      "Topic #3: state | upon | government | may | great | law | congress | public | united\n",
      "Topic #4: state | government | upon | congress | united | may | year | present | one\n",
      "Topic #5: state | government | year | congress | people | nation | united | great | country\n",
      "Topic #6: government | law | united | sight | invasion | withstand | bookkeeping | vacillation | state\n",
      "Topic #7: state | may | government | public | great | year | interest | war | make\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top words for topics in a topic model\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" | \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()    \n",
    "\n",
    "print_top_words(speech_lda, tfidf.columns.values, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
