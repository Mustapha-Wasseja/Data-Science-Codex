{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze state of the union addresses. \n",
    "Data source: https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents\n",
    "scrape the text of all speeches and then maybe try to find patterns of speech of each president?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Data source we are going to scrape for results\n",
    "data_url = 'https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents'\n",
    "\n",
    "link_list = []\n",
    "\n",
    "resp = urllib.request.urlopen(data_url)\n",
    "soup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "# Get all links to state of the union addresses from \n",
    "for link in soup.find_all('a', href=True):\n",
    "    if \"union_address\" in link['href'].lower() or \"union_speech\" in link['href'].lower() \\\n",
    "        and \"portal\" not in link['href'].lower() and \"#\" not in link['href'].lower():\n",
    "        link_list.append(link['href'])\n",
    "\n",
    "# extract the text of a speech from a URL\n",
    "# text is extracted in a list of paragraphs (strings) for each speech\n",
    "def get_speech(url):\n",
    "    return([ p.text.strip() for p in BeautifulSoup(urllib.request.urlopen(url)).find_all(\"p\") if \\\n",
    "             'This work is in the public domain in the United States because it is a work of the United States federal government' \\\n",
    "             not in p.text.strip()])\n",
    "\n",
    "speeches = [get_speech('https://en.wikisource.org' + link) for link in link_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract presidents names from link text\n",
    "presidents = [ link.replace('%','/').split('/')[2].replace('_',' ') for link in link_list ]\n",
    "\n",
    "# Extract state of the union text entries so we can extract the date\n",
    "sou_entries = []\n",
    "for item in soup.find_all('li'):\n",
    "    if 'union' in item.text.strip().lower() and '(' in  item.text.strip().lower():\n",
    "        sou_entries.append(item.text.strip())\n",
    "\n",
    "speeches_pd = pd.DataFrame({\n",
    "                'president' : presidents,\n",
    "                'speech' : speeches,\n",
    "                'year' : [int(re.findall('\\d+',item)[1]) for item in sou_entries ]} )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>speech</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[I embrace with great satisfaction the opportu...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow-Citizens of the Senate and the House o...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow-Citizens of the Senate and the House o...</td>\n",
       "      <td>1791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow-Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Fellow Citizens of the Senate and of the Hous...</td>\n",
       "      <td>1796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>John Adams</td>\n",
       "      <td>[I was for some time apprehensive that it woul...</td>\n",
       "      <td>1797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>John Adams</td>\n",
       "      <td>[Gentlemen of the Senate and Gentlemen of the ...</td>\n",
       "      <td>1798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           president                                             speech  year\n",
       "0  George Washington  [I embrace with great satisfaction the opportu...  1790\n",
       "1  George Washington  [Fellow-Citizens of the Senate and the House o...  1790\n",
       "2  George Washington  [Fellow-Citizens of the Senate and the House o...  1791\n",
       "3  George Washington  [Fellow-Citizens of the Senate and of the Hous...  1792\n",
       "4  George Washington  [Fellow Citizens of the Senate and of the Hous...  1793\n",
       "5  George Washington  [Fellow Citizens of the Senate and of the Hous...  1794\n",
       "6  George Washington  [Fellow Citizens of the Senate and of the Hous...  1795\n",
       "7  George Washington  [Fellow Citizens of the Senate and of the Hous...  1796\n",
       "8         John Adams  [I was for some time apprehensive that it woul...  1797\n",
       "9         John Adams  [Gentlemen of the Senate and Gentlemen of the ...  1798"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process and TfIdf Vectorize Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Convert nltk POS to POS that can be used by lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Lemmatize a word with its appropriate POS tag\n",
    "def lemmatize_with_pos(word,testing=False):\n",
    "    pos = get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "    if testing==True:\n",
    "        print('POS: ' + pos)\n",
    "    if pos != '':\n",
    "        return(lmtzr.lemmatize(word,pos))\n",
    "    else:\n",
    "        return(lmtzr.lemmatize(word))\n",
    "    \n",
    "MyStopWords = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS: n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dsfs'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_pos('dsfs',testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean, Tokenize, Lemmatize, and Vectorize all speeches \n",
    "speech_list = [\"\".join(speech) for speech in speeches_pd['speech'].tolist() ]\n",
    "\n",
    "tokens = [[lemmatize_with_pos(word.lower()) for word in word_tokenize(speech) if \\\n",
    "          word.isalpha() and word.lower() not in MyStopWords ] for \\\n",
    "          speech in speech_list ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(tvec.fit_transform([\" \".join(tok) for tok in tokens ]).toarray(),\n",
    "                    columns  = tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 23773)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA Topic Modeling\n",
    "speech_lda = LatentDirichletAllocation(n_components=5,\n",
    "                                    #   max_iter=50,\n",
    "                                learning_method='online')\n",
    "speech_lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top words for topics in a topic model\n",
    "def print_top_words(model, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" | \".join([model.columns.values[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()    \n",
    "\n",
    "print_top_words(speech_lda, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
